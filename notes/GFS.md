# The Google File System

***杂谈***

虽然论文中提到对以往在分布式文件系统设计领域中的一些假设, 观点之类的, 怎么样怎么样, 但是这篇论文对笔者来说是分布式文件系统的第一篇, 所以"旧时代"老旧的错误观点和假设我并不清楚, 毕竟, 即使是GFS这篇文章, 也是许多年前了的, 所以对于涉及到以前的设计错误之类, 笔者只能找到一个让自己觉得能认可的理由, 不会过深入地去了解.

## Abstract

这篇文章的成果: `GFS`, 文章的架构:

1. **接口**
2. **设计**
3. **性能**

我们能够从"接口"的内容中看见传统的文件系统扩展到支持分布式的文件系统的变化;

"设计"的内容则展示了接口的实现;

从这篇文章开始,我决定拾起以往忽略的文章中性能报告一部分,为了优化自己一直以来阅读论文的方式, 我会重新评判一下这是否有必要精读.

## Keywords

容错, 扩展性(兼容), 数据存储, 集群存储.

## Introduction

**Motivation:** 对数据处理的需求, 尤其是规模快速增长的数据.

**GFS**保持了传统分布式文件系统的要求: `性能`, `扩展性`, `可靠`, `可用`, 同时由于谷歌这种体量的公司对庞大的数据和计算集群有更多的机会观察, 所以对分布式文件系统有一些过时的假设, GFS不一样的地方就在这些方面.

1. *故障是常态*
2. *文件非常大*
3. *追加比修改更频繁*
4. *共同设计应用程序和文件系统API是有益的*

因此我们需要有一些应对的方案:

1. `持续监控`, `错误检测`, `容错`, `自动恢复`这些功能是必不可少的.
2. 需要重新设定一些`参数`(以往没有想象过文件会如此庞大).
3. 优化`追加操作`的性能, 保证追加操作的`原子性`是重点工作, `客户机中缓存文件内容`则变得没那么有吸引力.
4. 针对应用程序或者说工作负载来设计文件系统的接口, (当然)能极大提高特定负载下的文件系统性能.

我需要对第三点做补充, 由于工作负载的特点, 随机写入和覆盖数据没那么频繁, 而且读写的文件也非常庞大, 这意味着客户端读取了一部分数据很有可能不再读取, 那缓存的优势就没有那么大, 文件系统也不必为了缓存的一致性而增加更多复杂性, 我们可以把目光聚焦于如何让追加数据更快即可.

## Design

### Assumptions

1. *机器假设* : 故障是常态.
2. *文件大小假设* : 几百万个文件中, Multi-GB文件很常见, 尽管小文件也有,但是不必专门为它们进行优化.
3. *工作负载假设* :
   - 大的流式读取;
   - 小的随机读取;
   - 大的,顺序的追加写入,写入后很少再修改;
4. 多个客户端会同时追加同一个文件.
5. 应用程序大多重视高速批量处理数据, 对单个数据写入或读取的响应时间没有严格要求---高持续带宽 > 低延迟

笔者网络没怎么学, 单独解释一下**高持续带宽**, 描述的是数据传输的速度, 高带宽意味着可以在单位时间内传输更多的数据, 而**低延迟**指的是对一个数据的访问从发出申请到返回数据的间隔时间很小

### GFS's interfaces

常规的: `create`, `delete`, `open`, `close`, `read`, `write`
GFS特别提供: `snapshot`(快照), `record append`(追加记录)

具体细节在后续再说明.

### 架构

### chunk size

在当时, GFS设置的大小为64MB, 这对当时来说是一个比较大的数字,超过了大多数文件系统的块大小.

***Advantage***

1. 有效减小*clients*与*master*的交互次数;
2. 每个数据块信息更多, 与持久*TCP*搭配起来减少*TPC*连接的建立和关闭次数;
3. 追踪数据块所需的数据结构减少了, 从而*metadata*的大小变小, 这使得*master*可以将*metadata*保存在内存中, 这带来了另外的优势(见后文).

***Disadvantage***

对一个固定大小的文件来说, 较大的*size*意味着其数据将存储到更少的*chunkservers*上. 

如果文件被*clients*频繁地访问, 将导致一些*chunkservers*成为**hot spots** --- 即这些服务器将承受很大的负载.

让我举一个例子来说明一下: 

```
假设有1k个client访问文件A, 文件A的大小等于chunksize*chunknums Bytes, 每个chunkserver存储xxBytes,

1. chunksize = 64MB    chunknums = 1024  ServerNum = 16(假设分散存储)   AverAccessNum 62.5(63)
2. chunksize = 1024MB  chunknums = 64    ServerNum = 1                 AverAceessNum 1000
```

### Metadata

1. *file*和*chunk*的命名空间
2. *file-to-chunk*的映射信息
3. *chunk*副本的位置

对于前两种*metadata*, *master*采用持久化(写日志)的方式保存, 而每个*chunk*副本的位置, 则没有采取持久化的方式, 理由如下:

- 对于*chunk*保留在哪个*chunk server*, 这一信息GFS是动态获取的, 即当*master*重启或者有新的服务器加入集群时, *master*会询问服务器关于*chunk*副本的位置并保存该信息, 然后通过定时的心跳消息来监控服务器的状态.
- 如果我们想要持久化保存块位置信息, 那我们需要额外且大量的工作,去同步*chunkservers*与*master*的信息.
- 论文中提到的另一个理解方式: *chunkserver*对其磁盘上有哪些块有最终的决定权, 在*master*中保持一致视图是无意义的.

### Operation Log

操作日志: *master*会把对元数据的修改操作记录在其中, master用此来实现持久化的存储以及定义并行操作的具体逻辑顺序. 比如多个clients同时对一个文件进行追加操作, 究竟是哪一个先执行在日志中有明确的记录.

这部分日志内容除了存储在master本地磁盘中外,还会备份到多个远程机器上, 大部分写日志的优化都和操作系统中见过的相似, 比如批处理等等.

***对日志大小的限制***

这一限制的动机是为了避免重启*master*的重放操作的时间过长.

*operation log*和*checkpoint*技术搭配起来使用:当日志大小超过一定限制的时候, *master*就会进行加检查点, 然后清空日志的内容, 以便进行后续的操作记录, 一个简单的例子如下:

```
limit size---+----------+--------------
            -+         -+        .....
log size: -  .       -  .       - 
        -    .     -    .     -
      -      .   -      .   - 
    -        ↓ -        ↓-
time:--------+----------+-------------->
             A          B
        checkpoint  checkpoint
```



**值得注意的是**, 在上图`A`, `B`创建检查点的时候, 论文中提到对于拥有几百万个文件的集群来说, 这个时间开销在一分钟左右.

为了不阻塞这段时间内修改*metadata*的操作, 这个时候*master*会切换到新的日志文件, 然后在一个单独的线程中来进行创建检查点(意味着`A`, `B`时刻会有切换线程的操作)

设想我们在A~B之间某个时间点t1崩溃了, 那么我们可以按照**加载检查点A->重新执行[A~t1]内的操作**来恢复master的状态.

---
***小插曲: 2024/5/9***

这篇论文阅读搁浅了有一些久,中途重新阅读了一次笔记后去玩了两天游戏, **chaos;child**, 真好玩啊, 士力架我的超人!

如果这个月能完成lab3那六月份就奖励玩几天, 主要想打通每个线吧, 加油!

---

### 一致性模型

***写在前面的***

初读论文的时候我感觉到困惑, 因为我下意识的认为"绝对的透明"和"强一致性"是一个存储系统默认的特性.

在这篇论文对应的视频中, Robert教授解开了我的疑惑和不适感, Google的目标是构建一个大的, 快速的文件系统, 也就是*GFS*, *GFS*并不面向普通的用户, 这是一个Google内部使用的系统, 给内部工程师写应用程序使用的, 所以应用层对存储层具有一定的知识也就不足为奇, 其次*GFS*在各个方面对大型的顺序文件读写做了定制优化, 这意味着*GFS*是为了大型文件与性能而生.

阅读整篇论文后, 我是这么看待*GFS*的: 
- 对于大型文件采用追加操作, *GFS*提供了如同强一致性模型的保障;
- 其他情况采用随机读写, 此时*GFS*则视为弱一致性模型, 并不能保证返回正确的数据.

为此, *GFS*利用了很多技巧和假设来达到这一目的, 对追加操作有一致性保障的同时不损失太多性能

![GFS提供的一致性保障](../pic/GFS一致性.png)

**(1)** 对数据的修改操作有两种: *对已有数据的重写*; *追加数据*

**(2)** *GFS*保证了`(a)`对数据的每个副本修改顺序一致, 以及`(b)`使用*chunk*版本号

**(3)** 使用*GFS*系统编写应用程序的人会在应用层做出一些让步:
  1. 依赖追加操作
  2. *checkpoints*
  3. *self-validating*
  4. *self-identifying*

我们以











```c
int a[2];
// 同时发生以下两个线程:

//T1
a[0] = 0, a[1] = 0;

//T2
a[0] = 1, a[1] = 1;
```

对于弱一致性模型, 我们只能保证每个*clients*看到的`a[]`都是相同的, 但是对于上面两个修改,不能保证要么`a[0]=a[1]=1`, 要么`a[0]=a[1]=0`, 也就是说以下四个情况都是有可能的:

1. a[0] = 0 且 a[1] = 0;
2. a[0] = 1 且 a[1] = 1;
3. a[0] = 0 且 a[1] = 1;
4. a[0] = 1 且 a[1] = 0;

